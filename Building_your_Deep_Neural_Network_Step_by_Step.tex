\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Building\_your\_Deep\_Neural\_Network\_Step\_by\_Step}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{building-your-deep-neural-network-step-by-step}{%
\section{Building your Deep Neural Network: Step by
Step}\label{building-your-deep-neural-network-step-by-step}}

Welcome to your week 4 assignment (part 1 of 2)! Previously you trained
a 2-layer Neural Network with a single hidden layer. This week, you will
build a deep neural network with as many layers as you want!

\begin{itemize}
\tightlist
\item
  In this notebook, you'll implement all the functions required to build
  a deep neural network.
\item
  For the next assignment, you'll use these functions to build a deep
  neural network for image classification.
\end{itemize}

\textbf{By the end of this assignment, you'll be able to:}

\begin{itemize}
\tightlist
\item
  Use non-linear units like ReLU to improve your model
\item
  Build a deeper neural network (with more than 1 hidden layer)
\item
  Implement an easy-to-use neural network class
\end{itemize}

\textbf{Notation}: - Superscript \([l]\) denotes a quantity associated
with the \(l^{th}\) layer. - Example: \(a^{[L]}\) is the \(L^{th}\)
layer activation. \(W^{[L]}\) and \(b^{[L]}\) are the \(L^{th}\) layer
parameters. - Superscript \((i)\) denotes a quantity associated with the
\(i^{th}\) example. - Example: \(x^{(i)}\) is the \(i^{th}\) training
example. - Lowerscript \(i\) denotes the \(i^{th}\) entry of a vector. -
Example: \(a^{[l]}_i\) denotes the \(i^{th}\) entry of the \(l^{th}\)
layer's activations).

Let's get started!

    \hypertarget{table-of-contents}{%
\subsection{Table of Contents}\label{table-of-contents}}

\begin{itemize}
\tightlist
\item
  Section \ref{1}
\item
  Section \ref{2}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{3-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-1}
    \end{itemize}
  \item
    Section \ref{3-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-2}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{4-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-3}
    \end{itemize}
  \item
    Section \ref{4-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-4}
    \end{itemize}
  \item
    Section \ref{4-3}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-5}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{5}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex-6}
  \end{itemize}
\item
  Section \ref{6}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{6-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-7}
    \end{itemize}
  \item
    Section \ref{6-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-8}
    \end{itemize}
  \item
    Section \ref{6-3}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-9}
    \end{itemize}
  \item
    Section \ref{6-4}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-10}
    \end{itemize}
  \end{itemize}
\end{itemize}

    \#\# 1 - Packages

First, import all the packages you'll need during this assignment.

\begin{itemize}
\tightlist
\item
  \href{www.numpy.org}{numpy} is the main package for scientific
  computing with Python.
\item
  \href{http://matplotlib.org}{matplotlib} is a library to plot graphs
  in Python.
\item
  dnn\_utils provides some necessary functions for this notebook.
\item
  testCases provides some test cases to assess the correctness of your
  functions
\item
  np.random.seed(1) is used to keep all the random function calls
  consistent. It helps grade your work. Please don't change the seed!
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{h5py}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{testCases} \PY{k+kn}{import} \PY{o}{*}
\PY{k+kn}{from} \PY{n+nn}{dnn\PYZus{}utils} \PY{k+kn}{import} \PY{n}{sigmoid}\PY{p}{,} \PY{n}{sigmoid\PYZus{}backward}\PY{p}{,} \PY{n}{relu}\PY{p}{,} \PY{n}{relu\PYZus{}backward}
\PY{k+kn}{from} \PY{n+nn}{public\PYZus{}tests} \PY{k+kn}{import} \PY{o}{*}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{4.0}\PY{p}{)} \PY{c+c1}{\PYZsh{} set default size of plots}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image.interpolation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image.cmap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}

\PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
\PY{o}{\PYZpc{}}\PY{k}{autoreload} 2

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The autoreload extension is already loaded. To reload it, use:
  \%reload\_ext autoreload
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(4, 1)
\end{Verbatim}
\end{tcolorbox}
        
    \#\# 2 - Outline

To build your neural network, you'll be implementing several ``helper
functions.'' These helper functions will be used in the next assignment
to build a two-layer neural network and an L-layer neural network.

Each small helper function will have detailed instructions to walk you
through the necessary steps. Here's an outline of the steps in this
assignment:

\begin{itemize}
\tightlist
\item
  Initialize the parameters for a two-layer network and for an
  \(L\)-layer neural network
\item
  Implement the forward propagation module (shown in purple in the
  figure below)

  \begin{itemize}
  \tightlist
  \item
    Complete the LINEAR part of a layer's forward propagation step
    (resulting in \(Z^{[l]}\)).
  \item
    The ACTIVATION function is provided for you (relu/sigmoid)
  \item
    Combine the previous two steps into a new
    {[}LINEAR-\textgreater ACTIVATION{]} forward function.
  \item
    Stack the {[}LINEAR-\textgreater RELU{]} forward function L-1 time
    (for layers 1 through L-1) and add a
    {[}LINEAR-\textgreater SIGMOID{]} at the end (for the final layer
    \(L\)). This gives you a new L\_model\_forward function.
  \end{itemize}
\item
  Compute the loss
\item
  Implement the backward propagation module (denoted in red in the
  figure below)

  \begin{itemize}
  \tightlist
  \item
    Complete the LINEAR part of a layer's backward propagation step
  \item
    The gradient of the ACTIVATE function is provided for
    you(relu\_backward/sigmoid\_backward)
  \item
    Combine the previous two steps into a new
    {[}LINEAR-\textgreater ACTIVATION{]} backward function
  \item
    Stack {[}LINEAR-\textgreater RELU{]} backward L-1 times and add
    {[}LINEAR-\textgreater SIGMOID{]} backward in a new
    L\_model\_backward function
  \end{itemize}
\item
  Finally, update the parameters
\end{itemize}

Figure 1

\textbf{Note}:

For every forward function, there is a corresponding backward function.
This is why at every step of your forward module you will be storing
some values in a cache. These cached values are useful for computing
gradients.

In the backpropagation module, you can then use the cache to calculate
the gradients. Don't worry, this assignment will show you exactly how to
carry out each of these steps!

    \#\# 3 - Initialization

You will write two helper functions to initialize the parameters for
your model. The first function will be used to initialize parameters for
a two layer model. The second one generalizes this initialization
process to \(L\) layers.

\#\#\# 3.1 - 2-layer Neural Network

\#\#\# Exercise 1 - initialize\_parameters

Create and initialize the parameters of the 2-layer neural network.

\textbf{Instructions}:

\begin{itemize}
\tightlist
\item
  The model's structure is: \emph{LINEAR -\textgreater{} RELU
  -\textgreater{} LINEAR -\textgreater{} SIGMOID}.
\item
  Use this random initialization for the weight matrices:
  \texttt{np.random.randn(shape)*0.01} with the correct shape
\item
  Use zero initialization for the biases: \texttt{np.zeros(shape)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: initialize\PYZus{}parameters}

\PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{B} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{B}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{initialize\PYZus{}parameters}\PY{p}{(}\PY{n}{n\PYZus{}x}\PY{p}{,} \PY{n}{n\PYZus{}h}\PY{p}{,} \PY{n}{n\PYZus{}y}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Argument:}
\PY{l+s+sd}{    n\PYZus{}x \PYZhy{}\PYZhy{} size of the input layer}
\PY{l+s+sd}{    n\PYZus{}h \PYZhy{}\PYZhy{} size of the hidden layer}
\PY{l+s+sd}{    n\PYZus{}y \PYZhy{}\PYZhy{} size of the output layer}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} python dictionary containing your parameters:}
\PY{l+s+sd}{                    W1 \PYZhy{}\PYZhy{} weight matrix of shape (n\PYZus{}h, n\PYZus{}x)}
\PY{l+s+sd}{                    b1 \PYZhy{}\PYZhy{} bias vector of shape (n\PYZus{}h, 1)}
\PY{l+s+sd}{                    W2 \PYZhy{}\PYZhy{} weight matrix of shape (n\PYZus{}y, n\PYZus{}h)}
\PY{l+s+sd}{                    b2 \PYZhy{}\PYZhy{} bias vector of shape (n\PYZus{}y, 1)}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}(≈ 4 lines of code)}
    \PY{c+c1}{\PYZsh{} W1 = ...}
    \PY{c+c1}{\PYZsh{} b1 = ...}
    \PY{c+c1}{\PYZsh{} W2 = ...}
    \PY{c+c1}{\PYZsh{} b2 = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{W1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{n\PYZus{}h}\PY{p}{,} \PY{n}{n\PYZus{}x}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.01}
    \PY{n}{b1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{n\PYZus{}h}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{W2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{n\PYZus{}y}\PY{p}{,} \PY{n}{n\PYZus{}h}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.01}
    \PY{n}{b2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{n\PYZus{}y}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{W1}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{b1}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{W2}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{b2}\PY{p}{\PYZcb{}}
    
    \PY{k}{return} \PY{n}{parameters}    
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(4, 1)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{parameters} \PY{o}{=} \PY{n}{initialize\PYZus{}parameters}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{initialize\PYZus{}parameters\PYZus{}test}\PY{p}{(}\PY{n}{initialize\PYZus{}parameters}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
W1 = [[ 0.01624345 -0.00611756 -0.00528172]
 [-0.01072969  0.00865408 -0.02301539]]
b1 = [[0.]
 [0.]]
W2 = [[ 0.01744812 -0.00761207]]
b2 = [[0.]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{\emph{Expected output}}

\begin{verbatim}
W1 = [[ 0.01624345 -0.00611756 -0.00528172]
 [-0.01072969  0.00865408 -0.02301539]]
b1 = [[0.]
 [0.]]
W2 = [[ 0.01744812 -0.00761207]]
b2 = [[0.]]
\end{verbatim}

    \#\#\# 3.2 - L-layer Neural Network

The initialization for a deeper L-layer neural network is more
complicated because there are many more weight matrices and bias
vectors. When completing the \texttt{initialize\_parameters\_deep}
function, you should make sure that your dimensions match between each
layer. Recall that \(n^{[l]}\) is the number of units in layer \(l\).
For example, if the size of your input \(X\) is \((12288, 209)\) (with
\(m=209\) examples) then:

Shape of W

Shape of b

Activation

Shape of Activation

Layer 1

\((n^{[1]},12288)\)

\((n^{[1]},1)\)

\$Z\^{}\{{[}1{]}\} = W\^{}\{{[}1{]}\} X + b\^{}\{{[}1{]}\} \$

\((n^{[1]},209)\)

Layer 2

\((n^{[2]}, n^{[1]})\)

\((n^{[2]},1)\)

\(Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\)

\((n^{[2]}, 209)\)

\(\vdots\)

\(\vdots\)

\(\vdots\)

\(\vdots\)

\(\vdots\)

Layer L-1

\((n^{[L-1]}, n^{[L-2]})\)

\((n^{[L-1]}, 1)\)

\(Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]}\)

\((n^{[L-1]}, 209)\)

Layer L

\((n^{[L]}, n^{[L-1]})\)

\((n^{[L]}, 1)\)

\(Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]}\)

\((n^{[L]}, 209)\)

Remember that when you compute \(W X + b\) in python, it carries out
broadcasting. For example, if:

\[ W = \begin{bmatrix}
    w_{00}  & w_{01} & w_{02} \\
    w_{10}  & w_{11} & w_{12} \\
    w_{20}  & w_{21} & w_{22} 
\end{bmatrix}\;\;\; X = \begin{bmatrix}
    x_{00}  & x_{01} & x_{02} \\
    x_{10}  & x_{11} & x_{12} \\
    x_{20}  & x_{21} & x_{22} 
\end{bmatrix} \;\;\; b =\begin{bmatrix}
    b_0  \\
    b_1  \\
    b_2
\end{bmatrix}\tag{2}\]

Then \(WX + b\) will be:

\[ WX + b = \begin{bmatrix}
    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 & (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 & \cdots \\
    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 & (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 & \cdots \\
    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 & \cdots
\end{bmatrix}\tag{3}  \]

    \#\#\# Exercise 2 - initialize\_parameters\_deep

Implement initialization for an L-layer Neural Network.

\textbf{Instructions}: - The model's structure is \emph{{[}LINEAR
-\textgreater{} RELU{]} \$ \times\$ (L-1) -\textgreater{} LINEAR
-\textgreater{} SIGMOID}. I.e., it has \(L-1\) layers using a ReLU
activation function followed by an output layer with a sigmoid
activation function. - Use random initialization for the weight
matrices. Use \texttt{np.random.randn(shape)\ *\ 0.01}. - Use zeros
initialization for the biases. Use \texttt{np.zeros(shape)}. - You'll
store \(n^{[l]}\), the number of units in different layers, in a
variable \texttt{layer\_dims}. For example, the \texttt{layer\_dims} for
last week's Planar Data classification model would have been
{[}2,4,1{]}: There were two inputs, one hidden layer with 4 hidden
units, and an output layer with 1 output unit. This means \texttt{W1}'s
shape was (4,2), \texttt{b1} was (4,1), \texttt{W2} was (1,4) and
\texttt{b2} was (1,1). Now you will generalize this to \(L\) layers! -
Here is the implementation for \(L=1\) (one layer neural network). It
should inspire you to implement the general case (L-layer neural
network).

\begin{Shaded}
\begin{Highlighting}[]
    \ControlFlowTok{if}\NormalTok{ L }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
\NormalTok{        parameters[}\StringTok{"W"} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(L)] }\OperatorTok{=}\NormalTok{ np.random.randn(layer\_dims[}\DecValTok{1}\NormalTok{], layer\_dims[}\DecValTok{0}\NormalTok{]) }\OperatorTok{*} \FloatTok{0.01}
\NormalTok{        parameters[}\StringTok{"b"} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(L)] }\OperatorTok{=}\NormalTok{ np.zeros((layer\_dims[}\DecValTok{1}\NormalTok{], }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: initialize\PYZus{}parameters\PYZus{}deep}

\PY{k}{def} \PY{n+nf}{initialize\PYZus{}parameters\PYZus{}deep}\PY{p}{(}\PY{n}{layer\PYZus{}dims}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    layer\PYZus{}dims \PYZhy{}\PYZhy{} python array (list) containing the dimensions of each layer in our network}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} python dictionary containing your parameters \PYZdq{}W1\PYZdq{}, \PYZdq{}b1\PYZdq{}, ..., \PYZdq{}WL\PYZdq{}, \PYZdq{}bL\PYZdq{}:}
\PY{l+s+sd}{                    Wl \PYZhy{}\PYZhy{} weight matrix of shape (layer\PYZus{}dims[l], layer\PYZus{}dims[l\PYZhy{}1])}
\PY{l+s+sd}{                    bl \PYZhy{}\PYZhy{} bias vector of shape (layer\PYZus{}dims[l], 1)}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
    \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
    \PY{n}{L} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{layer\PYZus{}dims}\PY{p}{)} \PY{c+c1}{\PYZsh{} number of layers in the network}

    \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{L}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
        \PY{c+c1}{\PYZsh{} parameters[\PYZsq{}W\PYZsq{} + str(l)] = ...}
        \PY{c+c1}{\PYZsh{} parameters[\PYZsq{}b\PYZsq{} + str(l)] = ...}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{layer\PYZus{}dims}\PY{p}{[}\PY{n}{l}\PY{p}{]}\PY{p}{,} \PY{n}{layer\PYZus{}dims}\PY{p}{[}\PY{n}{l}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.01}
        \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{layer\PYZus{}dims}\PY{p}{[}\PY{n}{l}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
        
        \PY{k}{assert}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{n}{layer\PYZus{}dims}\PY{p}{[}\PY{n}{l}\PY{p}{]}\PY{p}{,} \PY{n}{layer\PYZus{}dims}\PY{p}{[}\PY{n}{l} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{k}{assert}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{n}{layer\PYZus{}dims}\PY{p}{[}\PY{n}{l}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

        
    \PY{k}{return} \PY{n}{parameters}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{parameters} \PY{o}{=} \PY{n}{initialize\PYZus{}parameters\PYZus{}deep}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{initialize\PYZus{}parameters\PYZus{}deep\PYZus{}test}\PY{p}{(}\PY{n}{initialize\PYZus{}parameters\PYZus{}deep}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]
 [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]
 [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]
 [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]
 [-0.01023785 -0.00712993  0.00625245 -0.00160513]
 [-0.00768836 -0.00230031  0.00745056  0.01976111]]
b2 = [[0.]
 [0.]
 [0.]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{\emph{Expected output}}

\begin{verbatim}
W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]
 [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]
 [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]
 [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]
 [-0.01023785 -0.00712993  0.00625245 -0.00160513]
 [-0.00768836 -0.00230031  0.00745056  0.01976111]]
b2 = [[0.]
 [0.]
 [0.]]
\end{verbatim}

    \#\# 4 - Forward Propagation Module

\#\#\# 4.1 - Linear Forward

Now that you have initialized your parameters, you can do the forward
propagation module. Start by implementing some basic functions that you
can use again later when implementing the model. Now, you'll complete
three functions in this order:

\begin{itemize}
\tightlist
\item
  LINEAR
\item
  LINEAR -\textgreater{} ACTIVATION where ACTIVATION will be either ReLU
  or Sigmoid.
\item
  {[}LINEAR -\textgreater{} RELU{]} \(\times\) (L-1) -\textgreater{}
  LINEAR -\textgreater{} SIGMOID (whole model)
\end{itemize}

The linear forward module (vectorized over all the examples) computes
the following equations:

\[Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{4}\]

where \(A^{[0]} = X\).

\#\#\# Exercise 3 - linear\_forward

Build the linear part of forward propagation.

\textbf{Reminder}: The mathematical representation of this unit is
\(Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\). You may also find
\texttt{np.dot()} useful. If your dimensions don't match, printing
\texttt{W.shape} may help.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: linear\PYZus{}forward}

\PY{k}{def} \PY{n+nf}{linear\PYZus{}forward}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement the linear part of a layer\PYZsq{}s forward propagation.}

\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    A \PYZhy{}\PYZhy{} activations from previous layer (or input data): (size of previous layer, number of examples)}
\PY{l+s+sd}{    W \PYZhy{}\PYZhy{} weights matrix: numpy array of shape (size of current layer, size of previous layer)}
\PY{l+s+sd}{    b \PYZhy{}\PYZhy{} bias vector, numpy array of shape (size of the current layer, 1)}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    Z \PYZhy{}\PYZhy{} the input of the activation function, also called pre\PYZhy{}activation parameter }
\PY{l+s+sd}{    cache \PYZhy{}\PYZhy{} a python tuple containing \PYZdq{}A\PYZdq{}, \PYZdq{}W\PYZdq{} and \PYZdq{}b\PYZdq{} ; stored for computing the backward pass efficiently}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}(≈ 1 line of code)}
    \PY{c+c1}{\PYZsh{} Z = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{Z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W}\PY{p}{,}\PY{n}{A}\PY{p}{)} \PY{o}{+} \PY{n}{b}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{cache} \PY{o}{=} \PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{Z}\PY{p}{,} \PY{n}{cache}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}A}\PY{p}{,} \PY{n}{t\PYZus{}W}\PY{p}{,} \PY{n}{t\PYZus{}b} \PY{o}{=} \PY{n}{linear\PYZus{}forward\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}
\PY{n}{t\PYZus{}Z}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}cache} \PY{o}{=} \PY{n}{linear\PYZus{}forward}\PY{p}{(}\PY{n}{t\PYZus{}A}\PY{p}{,} \PY{n}{t\PYZus{}W}\PY{p}{,} \PY{n}{t\PYZus{}b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}Z}\PY{p}{)}\PY{p}{)}

\PY{n}{linear\PYZus{}forward\PYZus{}test}\PY{p}{(}\PY{n}{linear\PYZus{}forward}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Z = [[ 3.26295337 -1.23429987]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{\emph{Expected output}}

\begin{verbatim}
Z = [[ 3.26295337 -1.23429987]]
\end{verbatim}

    \#\#\# 4.2 - Linear-Activation Forward

In this notebook, you will use two activation functions:

\begin{itemize}
\tightlist
\item
  \textbf{Sigmoid}:
  \(\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)}}\).
  You've been provided with the \texttt{sigmoid} function which returns
  \textbf{two} items: the activation value ``\texttt{a}'' and a
  ``\texttt{cache}'' that contains ``\texttt{Z}'' (it's what we will
  feed in to the corresponding backward function). To use it you could
  just call:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A, activation\_cache }\OperatorTok{=}\NormalTok{ sigmoid(Z)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{ReLU}: The mathematical formula for ReLu is
  \(A = RELU(Z) = max(0, Z)\). You've been provided with the
  \texttt{relu} function. This function returns \textbf{two} items: the
  activation value ``\texttt{A}'' and a ``\texttt{cache}'' that contains
  ``\texttt{Z}'' (it's what you'll feed in to the corresponding backward
  function). To use it you could just call:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A, activation\_cache }\OperatorTok{=}\NormalTok{ relu(Z)}
\end{Highlighting}
\end{Shaded}

    For added convenience, you're going to group two functions (Linear and
Activation) into one function (LINEAR-\textgreater ACTIVATION). Hence,
you'll implement a function that does the LINEAR forward step, followed
by an ACTIVATION forward step.

\#\#\# Exercise 4 - linear\_activation\_forward

Implement the forward propagation of the
\emph{LINEAR-\textgreater ACTIVATION} layer. Mathematical relation is:
\(A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\) where the
activation ``g'' can be sigmoid() or relu(). Use
\texttt{linear\_forward()} and the correct activation function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: linear\PYZus{}activation\PYZus{}forward}

\PY{k}{def} \PY{n+nf}{linear\PYZus{}activation\PYZus{}forward}\PY{p}{(}\PY{n}{A\PYZus{}prev}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{activation}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement the forward propagation for the LINEAR\PYZhy{}\PYZgt{}ACTIVATION layer}

\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    A\PYZus{}prev \PYZhy{}\PYZhy{} activations from previous layer (or input data): (size of previous layer, number of examples)}
\PY{l+s+sd}{    W \PYZhy{}\PYZhy{} weights matrix: numpy array of shape (size of current layer, size of previous layer)}
\PY{l+s+sd}{    b \PYZhy{}\PYZhy{} bias vector, numpy array of shape (size of the current layer, 1)}
\PY{l+s+sd}{    activation \PYZhy{}\PYZhy{} the activation to be used in this layer, stored as a text string: \PYZdq{}sigmoid\PYZdq{} or \PYZdq{}relu\PYZdq{}}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    A \PYZhy{}\PYZhy{} the output of the activation function, also called the post\PYZhy{}activation value }
\PY{l+s+sd}{    cache \PYZhy{}\PYZhy{} a python tuple containing \PYZdq{}linear\PYZus{}cache\PYZdq{} and \PYZdq{}activation\PYZus{}cache\PYZdq{};}
\PY{l+s+sd}{             stored for computing the backward pass efficiently}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{k}{if} \PY{n}{activation} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
        \PY{c+c1}{\PYZsh{} Z, linear\PYZus{}cache = ...}
        \PY{c+c1}{\PYZsh{} A, activation\PYZus{}cache = ...}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{Z}\PY{p}{,} \PY{n}{linear\PYZus{}cache} \PY{o}{=} \PY{n}{linear\PYZus{}forward}\PY{p}{(}\PY{n}{A\PYZus{}prev}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}
        \PY{n}{A}\PY{p}{,} \PY{n}{activation\PYZus{}cache} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{Z}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{k}{elif} \PY{n}{activation} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
        \PY{c+c1}{\PYZsh{} Z, linear\PYZus{}cache = ...}
        \PY{c+c1}{\PYZsh{} A, activation\PYZus{}cache = ...}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{Z}\PY{p}{,} \PY{n}{linear\PYZus{}cache} \PY{o}{=} \PY{n}{linear\PYZus{}forward}\PY{p}{(}\PY{n}{A\PYZus{}prev}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}
        \PY{n}{A}\PY{p}{,} \PY{n}{activation\PYZus{}cache} \PY{o}{=} \PY{n}{relu}\PY{p}{(}\PY{n}{Z}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{cache} \PY{o}{=} \PY{p}{(}\PY{n}{linear\PYZus{}cache}\PY{p}{,} \PY{n}{activation\PYZus{}cache}\PY{p}{)}

    \PY{k}{return} \PY{n}{A}\PY{p}{,} \PY{n}{cache}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}A\PYZus{}prev}\PY{p}{,} \PY{n}{t\PYZus{}W}\PY{p}{,} \PY{n}{t\PYZus{}b} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}forward\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}

\PY{n}{t\PYZus{}A}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}activation\PYZus{}cache} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}forward}\PY{p}{(}\PY{n}{t\PYZus{}A\PYZus{}prev}\PY{p}{,} \PY{n}{t\PYZus{}W}\PY{p}{,} \PY{n}{t\PYZus{}b}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With sigmoid: A = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}A}\PY{p}{)}\PY{p}{)}

\PY{n}{t\PYZus{}A}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}activation\PYZus{}cache} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}forward}\PY{p}{(}\PY{n}{t\PYZus{}A\PYZus{}prev}\PY{p}{,} \PY{n}{t\PYZus{}W}\PY{p}{,} \PY{n}{t\PYZus{}b}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With ReLU: A = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}A}\PY{p}{)}\PY{p}{)}

\PY{n}{linear\PYZus{}activation\PYZus{}forward\PYZus{}test}\PY{p}{(}\PY{n}{linear\PYZus{}activation\PYZus{}forward}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
With sigmoid: A = [[0.96890023 0.11013289]]
With ReLU: A = [[3.43896131 0.        ]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{\emph{Expected output}}

\begin{verbatim}
With sigmoid: A = [[0.96890023 0.11013289]]
With ReLU: A = [[3.43896131 0.        ]]
\end{verbatim}

    \textbf{Note}: In deep learning, the
``{[}LINEAR-\textgreater ACTIVATION{]}'' computation is counted as a
single layer in the neural network, not two layers.

    \#\#\# 4.3 - L-Layer Model

For even \emph{more} convenience when implementing the \(L\)-layer
Neural Net, you will need a function that replicates the previous one
(\texttt{linear\_activation\_forward} with RELU) \(L-1\) times, then
follows that with one \texttt{linear\_activation\_forward} with SIGMOID.

Figure 2 : \emph{{[}LINEAR -\textgreater{} RELU{]} \(\times\) (L-1)
-\textgreater{} LINEAR -\textgreater{} SIGMOID} model

\#\#\# Exercise 5 - L\_model\_forward

Implement the forward propagation of the above model.

\textbf{Instructions}: In the code below, the variable \texttt{AL} will
denote
\(A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]} A^{[L-1]} + b^{[L]})\).
(This is sometimes also called \texttt{Yhat}, i.e., this is
\(\hat{Y}\).)

\textbf{Hints}: - Use the functions you've previously written - Use a
for loop to replicate {[}LINEAR-\textgreater RELU{]} (L-1) times - Don't
forget to keep track of the caches in the ``caches'' list. To add a new
value \texttt{c} to a \texttt{list}, you can use
\texttt{list.append(c)}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: L\PYZus{}model\PYZus{}forward}

\PY{k}{def} \PY{n+nf}{L\PYZus{}model\PYZus{}forward}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement forward propagation for the [LINEAR\PYZhy{}\PYZgt{}RELU]*(L\PYZhy{}1)\PYZhy{}\PYZgt{}LINEAR\PYZhy{}\PYZgt{}SIGMOID computation}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    X \PYZhy{}\PYZhy{} data, numpy array of shape (input size, number of examples)}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} output of initialize\PYZus{}parameters\PYZus{}deep()}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    AL \PYZhy{}\PYZhy{} activation value from the output (last) layer}
\PY{l+s+sd}{    caches \PYZhy{}\PYZhy{} list of caches containing:}
\PY{l+s+sd}{                every cache of linear\PYZus{}activation\PYZus{}forward() (there are L of them, indexed from 0 to L\PYZhy{}1)}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{n}{caches} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{A} \PY{o}{=} \PY{n}{X}
    \PY{n}{L} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{parameters}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}                  \PY{c+c1}{\PYZsh{} number of layers in the neural network}
    
    \PY{c+c1}{\PYZsh{} Implement [LINEAR \PYZhy{}\PYZgt{} RELU]*(L\PYZhy{}1). Add \PYZdq{}cache\PYZdq{} to the \PYZdq{}caches\PYZdq{} list.}
    \PY{c+c1}{\PYZsh{} The for loop starts at 1 because layer 0 is the input}
    \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{L}\PY{p}{)}\PY{p}{:}
        \PY{n}{A\PYZus{}prev} \PY{o}{=} \PY{n}{A} 
        \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
        \PY{c+c1}{\PYZsh{} A, cache = ...}
        \PY{c+c1}{\PYZsh{} caches ...}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{A}\PY{p}{,} \PY{n}{cache} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}forward}\PY{p}{(}\PY{n}{A\PYZus{}prev}\PY{p}{,} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{caches}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cache}\PY{p}{)}\PY{p}{;}
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{c+c1}{\PYZsh{} Implement LINEAR \PYZhy{}\PYZgt{} SIGMOID. Add \PYZdq{}cache\PYZdq{} to the \PYZdq{}caches\PYZdq{} list.}
    \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
    \PY{c+c1}{\PYZsh{} AL, cache = ...}
    \PY{c+c1}{\PYZsh{} caches ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{AL}\PY{p}{,} \PY{n}{cache} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}forward}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{L}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{L}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{caches}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cache}\PY{p}{)}\PY{p}{;}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
          
    \PY{k}{return} \PY{n}{AL}\PY{p}{,} \PY{n}{caches}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{t\PYZus{}parameters} \PY{o}{=} \PY{n}{L\PYZus{}model\PYZus{}forward\PYZus{}test\PYZus{}case\PYZus{}2hidden}\PY{p}{(}\PY{p}{)}
\PY{n}{t\PYZus{}AL}\PY{p}{,} \PY{n}{t\PYZus{}caches} \PY{o}{=} \PY{n}{L\PYZus{}model\PYZus{}forward}\PY{p}{(}\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{t\PYZus{}parameters}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AL = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}AL}\PY{p}{)}\PY{p}{)}

\PY{n}{L\PYZus{}model\PYZus{}forward\PYZus{}test}\PY{p}{(}\PY{n}{L\PYZus{}model\PYZus{}forward}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[-0.31178367  0.72900392  0.21782079 -0.8990918 ]
 [-2.48678065  0.91325152  1.12706373 -1.51409323]
 [ 1.63929108 -0.4298936   2.63128056  0.60182225]
 [-0.33588161  1.23773784  0.11112817  0.12915125]
 [ 0.07612761 -0.15512816  0.63422534  0.810655  ]]
\{'W1': array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],
       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],
       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],
       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),
'b1': array([[ 1.38503523],
       [-0.51962709],
       [-0.78015214],
       [ 0.95560959]]), 'W2': array([[-0.12673638, -1.36861282,  1.21848065,
-0.85750144],
       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],
       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), 'b2': array([[
1.50278553],
       [-0.59545972],
       [ 0.52834106]]), 'W3': array([[ 0.9398248 ,  0.42628539, -0.75815703]]),
'b3': array([[-0.16236698]])\}
AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]
[[-0.31178367  0.72900392  0.21782079 -0.8990918 ]
 [-2.48678065  0.91325152  1.12706373 -1.51409323]
 [ 1.63929108 -0.4298936   2.63128056  0.60182225]
 [-0.33588161  1.23773784  0.11112817  0.12915125]
 [ 0.07612761 -0.15512816  0.63422534  0.810655  ]]
\{'W1': array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],
       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],
       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],
       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),
'b1': array([[ 1.38503523],
       [-0.51962709],
       [-0.78015214],
       [ 0.95560959]]), 'W2': array([[-0.12673638, -1.36861282,  1.21848065,
-0.85750144],
       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],
       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), 'b2': array([[
1.50278553],
       [-0.59545972],
       [ 0.52834106]]), 'W3': array([[ 0.9398248 ,  0.42628539, -0.75815703]]),
'b3': array([[-0.16236698]])\}
[[-0.31178367  0.72900392  0.21782079 -0.8990918 ]
 [-2.48678065  0.91325152  1.12706373 -1.51409323]
 [ 1.63929108 -0.4298936   2.63128056  0.60182225]
 [-0.33588161  1.23773784  0.11112817  0.12915125]
 [ 0.07612761 -0.15512816  0.63422534  0.810655  ]]
\{'W1': array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],
       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],
       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],
       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),
'b1': array([[ 1.38503523],
       [-0.51962709],
       [-0.78015214],
       [ 0.95560959]]), 'W2': array([[-0.12673638, -1.36861282,  1.21848065,
-0.85750144],
       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],
       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), 'b2': array([[
1.50278553],
       [-0.59545972],
       [ 0.52834106]]), 'W3': array([[ 0.9398248 ,  0.42628539, -0.75815703]]),
'b3': array([[-0.16236698]])\}
[[-0.31178367  0.72900392  0.21782079 -0.8990918 ]
 [-2.48678065  0.91325152  1.12706373 -1.51409323]
 [ 1.63929108 -0.4298936   2.63128056  0.60182225]
 [-0.33588161  1.23773784  0.11112817  0.12915125]
 [ 0.07612761 -0.15512816  0.63422534  0.810655  ]]
\{'W1': array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],
       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],
       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],
       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),
'b1': array([[ 1.38503523],
       [-0.51962709],
       [-0.78015214],
       [ 0.95560959]]), 'W2': array([[-0.12673638, -1.36861282,  1.21848065,
-0.85750144],
       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],
       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), 'b2': array([[
1.50278553],
       [-0.59545972],
       [ 0.52834106]]), 'W3': array([[ 0.9398248 ,  0.42628539, -0.75815703]]),
'b3': array([[-0.16236698]])\}
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{\emph{Expected output}}

\begin{verbatim}
AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]
\end{verbatim}

    \textbf{Awesome!} You've implemented a full forward propagation that
takes the input X and outputs a row vector \(A^{[L]}\) containing your
predictions. It also records all intermediate values in ``caches''.
Using \(A^{[L]}\), you can compute the cost of your predictions.

    \#\# 5 - Cost Function

Now you can implement forward and backward propagation! You need to
compute the cost, in order to check whether your model is actually
learning.

\#\#\# Exercise 6 - compute\_cost Compute the cross-entropy cost \(J\),
using the following formula:
\[-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}cost}

\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{AL}\PY{p}{,} \PY{n}{Y}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement the cost function defined by equation (7).}

\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    AL \PYZhy{}\PYZhy{} probability vector corresponding to your label predictions, shape (1, number of examples)}
\PY{l+s+sd}{    Y \PYZhy{}\PYZhy{} true \PYZdq{}label\PYZdq{} vector (for example: containing 0 if non\PYZhy{}cat, 1 if cat), shape (1, number of examples)}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    cost \PYZhy{}\PYZhy{} cross\PYZhy{}entropy cost}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{m} \PY{o}{=} \PY{n}{Y}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Compute loss from aL and y.}
    \PY{c+c1}{\PYZsh{} (≈ 1 lines of code)}
    \PY{c+c1}{\PYZsh{} cost = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{cost} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{AL}\PY{p}{)}\PY{p}{,} \PY{n}{Y}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{AL}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{Y}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{n}{cost} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{cost}\PY{p}{)}      \PY{c+c1}{\PYZsh{} To make sure your cost\PYZsq{}s shape is what we expect (e.g. this turns [[17]] into 17).}

    
    \PY{k}{return} \PY{n}{cost}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}Y}\PY{p}{,} \PY{n}{t\PYZus{}AL} \PY{o}{=} \PY{n}{compute\PYZus{}cost\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}
\PY{n}{t\PYZus{}cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{t\PYZus{}AL}\PY{p}{,} \PY{n}{t\PYZus{}Y}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cost: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}cost}\PY{p}{)}\PY{p}{)}

\PY{n}{compute\PYZus{}cost\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}cost}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cost: 0.2797765635793422
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{Expected Output}:

cost

0.2797765635793422

    \#\# 6 - Backward Propagation Module

Just as you did for the forward propagation, you'll implement helper
functions for backpropagation. Remember that backpropagation is used to
calculate the gradient of the loss function with respect to the
parameters.

\textbf{Reminder}:

Figure 3: Forward and Backward propagation for
LINEAR-\textgreater RELU-\textgreater LINEAR-\textgreater SIGMOID The
purple blocks represent the forward propagation, and the red blocks
represent the backward propagation.

Now, similarly to forward propagation, you're going to build the
backward propagation in three steps: 1. LINEAR backward 2. LINEAR
-\textgreater{} ACTIVATION backward where ACTIVATION computes the
derivative of either the ReLU or sigmoid activation 3. {[}LINEAR
-\textgreater{} RELU{]} \(\times\) (L-1) -\textgreater{} LINEAR
-\textgreater{} SIGMOID backward (whole model)

    For the next exercise, you will need to remember that:

\begin{itemize}
\tightlist
\item
  \texttt{b} is a matrix(np.ndarray) with 1 column and n rows, i.e: b =
  {[}{[}1.0{]}, {[}2.0{]}{]} (remember that \texttt{b} is a constant)
\item
  np.sum performs a sum over the elements of a ndarray
\item
  axis=1 or axis=0 specify if the sum is carried out by rows or by
  columns respectively
\item
  keepdims specifies if the original dimensions of the matrix must be
  kept.
\item
  Look at the following example to clarify:
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{]}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{axis=1 and keepdims=True}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{axis=1 and keepdims=False}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{axis=0 and keepdims=True}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{axis=0 and keepdims=False}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
axis=1 and keepdims=True
[[3]
 [7]]
axis=1 and keepdims=False
[3 7]
axis=0 and keepdims=True
[[4 6]]
axis=0 and keepdims=False
[4 6]
    \end{Verbatim}

    \#\#\# 6.1 - Linear Backward

For layer \(l\), the linear part is:
\(Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\) (followed by an activation).

Suppose you have already calculated the derivative
\(dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}\). You want
to get \((dW^{[l]}, db^{[l]}, dA^{[l-1]})\).

Figure 4

The three outputs \((dW^{[l]}, db^{[l]}, dA^{[l-1]})\) are computed
using the input \(dZ^{[l]}\).

Here are the formulas you need:
\[ dW^{[l]} = \frac{\partial \mathcal{J} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{8}\]
\[ db^{[l]} = \frac{\partial \mathcal{J} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\tag{9}\]
\[ dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{10}\]

\(A^{[l-1] T}\) is the transpose of \(A^{[l-1]}\).

    \#\#\# Exercise 7 - linear\_backward

Use the 3 formulas above to implement \texttt{linear\_backward()}.

\textbf{Hint}:

\begin{itemize}
\tightlist
\item
  In numpy you can get the transpose of an ndarray \texttt{A} using
  \texttt{A.T} or \texttt{A.transpose()}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: linear\PYZus{}backward}

\PY{k}{def} \PY{n+nf}{linear\PYZus{}backward}\PY{p}{(}\PY{n}{dZ}\PY{p}{,} \PY{n}{cache}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement the linear portion of backward propagation for a single layer (layer l)}

\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    dZ \PYZhy{}\PYZhy{} Gradient of the cost with respect to the linear output (of current layer l)}
\PY{l+s+sd}{    cache \PYZhy{}\PYZhy{} tuple of values (A\PYZus{}prev, W, b) coming from the forward propagation in the current layer}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    dA\PYZus{}prev \PYZhy{}\PYZhy{} Gradient of the cost with respect to the activation (of the previous layer l\PYZhy{}1), same shape as A\PYZus{}prev}
\PY{l+s+sd}{    dW \PYZhy{}\PYZhy{} Gradient of the cost with respect to W (current layer l), same shape as W}
\PY{l+s+sd}{    db \PYZhy{}\PYZhy{} Gradient of the cost with respect to b (current layer l), same shape as b}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{A\PYZus{}prev}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{cache}
    \PY{n}{m} \PY{o}{=} \PY{n}{A\PYZus{}prev}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} (≈ 3 lines of code)}
    \PY{c+c1}{\PYZsh{} dW = ...}
    \PY{c+c1}{\PYZsh{} db = ... sum by the rows of dZ with keepdims=True}
    \PY{c+c1}{\PYZsh{} dA\PYZus{}prev = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{dW} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dZ}\PY{p}{,} \PY{n}{A\PYZus{}prev}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{o}{/} \PY{n}{m}
    \PY{n}{db} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dZ}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)} \PY{o}{/} \PY{n}{m}
    \PY{n}{dA\PYZus{}prev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{dZ}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{k}{return} \PY{n}{dA\PYZus{}prev}\PY{p}{,} \PY{n}{dW}\PY{p}{,} \PY{n}{db}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}dZ}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}cache} \PY{o}{=} \PY{n}{linear\PYZus{}backward\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}
\PY{n}{t\PYZus{}dA\PYZus{}prev}\PY{p}{,} \PY{n}{t\PYZus{}dW}\PY{p}{,} \PY{n}{t\PYZus{}db} \PY{o}{=} \PY{n}{linear\PYZus{}backward}\PY{p}{(}\PY{n}{t\PYZus{}dZ}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}cache}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA\PYZus{}prev: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}dA\PYZus{}prev}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}dW}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}db}\PY{p}{)}\PY{p}{)}

\PY{n}{linear\PYZus{}backward\PYZus{}test}\PY{p}{(}\PY{n}{linear\PYZus{}backward}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dA\_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]
 [ 0.60345879 -3.72508701  5.81700741 -3.84326836]
 [-0.4319552  -1.30987417  1.72354705  0.05070578]
 [-0.38981415  0.60811244 -1.25938424  1.47191593]
 [-2.52214926  2.67882552 -0.67947465  1.48119548]]
dW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]
 [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]
 [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]
db: [[-0.14713786]
 [-0.11313155]
 [-0.13209101]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{Expected Output}:

\begin{verbatim}
dA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]
 [ 0.60345879 -3.72508701  5.81700741 -3.84326836]
 [-0.4319552  -1.30987417  1.72354705  0.05070578]
 [-0.38981415  0.60811244 -1.25938424  1.47191593]
 [-2.52214926  2.67882552 -0.67947465  1.48119548]]
dW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]
 [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]
 [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]
db: [[-0.14713786]
 [-0.11313155]
 [-0.13209101]]
\end{verbatim}

    \#\#\# 6.2 - Linear-Activation Backward

Next, you will create a function that merges the two helper functions:
\textbf{\texttt{linear\_backward}} and the backward step for the
activation \textbf{\texttt{linear\_activation\_backward}}.

To help you implement \texttt{linear\_activation\_backward}, two
backward functions have been provided: -
\textbf{\texttt{sigmoid\_backward}}: Implements the backward propagation
for SIGMOID unit. You can call it as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dZ }\OperatorTok{=}\NormalTok{ sigmoid\_backward(dA, activation\_cache)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{relu\_backward}}: Implements the backward propagation
  for RELU unit. You can call it as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dZ }\OperatorTok{=}\NormalTok{ relu\_backward(dA, activation\_cache)}
\end{Highlighting}
\end{Shaded}

If \(g(.)\) is the activation function, \texttt{sigmoid\_backward} and
\texttt{relu\_backward} compute
\[dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). \tag{11}\]

\#\#\# Exercise 8 - linear\_activation\_backward

Implement the backpropagation for the
\emph{LINEAR-\textgreater ACTIVATION} layer.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: linear\PYZus{}activation\PYZus{}backward}

\PY{k}{def} \PY{n+nf}{linear\PYZus{}activation\PYZus{}backward}\PY{p}{(}\PY{n}{dA}\PY{p}{,} \PY{n}{cache}\PY{p}{,} \PY{n}{activation}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement the backward propagation for the LINEAR\PYZhy{}\PYZgt{}ACTIVATION layer.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    dA \PYZhy{}\PYZhy{} post\PYZhy{}activation gradient for current layer l }
\PY{l+s+sd}{    cache \PYZhy{}\PYZhy{} tuple of values (linear\PYZus{}cache, activation\PYZus{}cache) we store for computing backward propagation efficiently}
\PY{l+s+sd}{    activation \PYZhy{}\PYZhy{} the activation to be used in this layer, stored as a text string: \PYZdq{}sigmoid\PYZdq{} or \PYZdq{}relu\PYZdq{}}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    dA\PYZus{}prev \PYZhy{}\PYZhy{} Gradient of the cost with respect to the activation (of the previous layer l\PYZhy{}1), same shape as A\PYZus{}prev}
\PY{l+s+sd}{    dW \PYZhy{}\PYZhy{} Gradient of the cost with respect to W (current layer l), same shape as W}
\PY{l+s+sd}{    db \PYZhy{}\PYZhy{} Gradient of the cost with respect to b (current layer l), same shape as b}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{linear\PYZus{}cache}\PY{p}{,} \PY{n}{activation\PYZus{}cache} \PY{o}{=} \PY{n}{cache}
    
    \PY{k}{if} \PY{n}{activation} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
        \PY{c+c1}{\PYZsh{} dZ =  ...}
        \PY{c+c1}{\PYZsh{} dA\PYZus{}prev, dW, db =  ...}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{dZ} \PY{o}{=} \PY{n}{relu\PYZus{}backward}\PY{p}{(}\PY{n}{dA}\PY{p}{,} \PY{n}{activation\PYZus{}cache}\PY{p}{)}
        \PY{n}{dA\PYZus{}prev}\PY{p}{,} \PY{n}{dW}\PY{p}{,} \PY{n}{db} \PY{o}{=} \PY{n}{linear\PYZus{}backward}\PY{p}{(}\PY{n}{dZ}\PY{p}{,} \PY{n}{linear\PYZus{}cache}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
        
    \PY{k}{elif} \PY{n}{activation} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
        \PY{c+c1}{\PYZsh{} dZ =  ...}
        \PY{c+c1}{\PYZsh{} dA\PYZus{}prev, dW, db =  ...}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{dZ} \PY{o}{=} \PY{n}{sigmoid\PYZus{}backward}\PY{p}{(}\PY{n}{dA}\PY{p}{,} \PY{n}{activation\PYZus{}cache}\PY{p}{)}
        \PY{n}{dA\PYZus{}prev}\PY{p}{,} \PY{n}{dW}\PY{p}{,} \PY{n}{db} \PY{o}{=} \PY{n}{linear\PYZus{}backward}\PY{p}{(}\PY{n}{dZ}\PY{p}{,} \PY{n}{linear\PYZus{}cache}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{k}{return} \PY{n}{dA\PYZus{}prev}\PY{p}{,} \PY{n}{dW}\PY{p}{,} \PY{n}{db}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}dAL}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}activation\PYZus{}cache} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}backward\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}

\PY{n}{t\PYZus{}dA\PYZus{}prev}\PY{p}{,} \PY{n}{t\PYZus{}dW}\PY{p}{,} \PY{n}{t\PYZus{}db} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}backward}\PY{p}{(}\PY{n}{t\PYZus{}dAL}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}activation\PYZus{}cache}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With sigmoid: dA\PYZus{}prev = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}dA\PYZus{}prev}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With sigmoid: dW = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}dW}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With sigmoid: db = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}db}\PY{p}{)}\PY{p}{)}

\PY{n}{t\PYZus{}dA\PYZus{}prev}\PY{p}{,} \PY{n}{t\PYZus{}dW}\PY{p}{,} \PY{n}{t\PYZus{}db} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}backward}\PY{p}{(}\PY{n}{t\PYZus{}dAL}\PY{p}{,} \PY{n}{t\PYZus{}linear\PYZus{}activation\PYZus{}cache}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With relu: dA\PYZus{}prev = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}dA\PYZus{}prev}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With relu: dW = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}dW}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{With relu: db = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}db}\PY{p}{)}\PY{p}{)}

\PY{n}{linear\PYZus{}activation\PYZus{}backward\PYZus{}test}\PY{p}{(}\PY{n}{linear\PYZus{}activation\PYZus{}backward}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
With sigmoid: dA\_prev = [[ 0.11017994  0.01105339]
 [ 0.09466817  0.00949723]
 [-0.05743092 -0.00576154]]
With sigmoid: dW = [[ 0.10266786  0.09778551 -0.01968084]]
With sigmoid: db = [[-0.05729622]]
With relu: dA\_prev = [[ 0.44090989  0.        ]
 [ 0.37883606  0.        ]
 [-0.2298228   0.        ]]
With relu: dW = [[ 0.44513824  0.37371418 -0.10478989]]
With relu: db = [[-0.20837892]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{Expected output:}

\begin{verbatim}
With sigmoid: dA_prev = [[ 0.11017994  0.01105339]
 [ 0.09466817  0.00949723]
 [-0.05743092 -0.00576154]]
With sigmoid: dW = [[ 0.10266786  0.09778551 -0.01968084]]
With sigmoid: db = [[-0.05729622]]
With relu: dA_prev = [[ 0.44090989  0.        ]
 [ 0.37883606  0.        ]
 [-0.2298228   0.        ]]
With relu: dW = [[ 0.44513824  0.37371418 -0.10478989]]
With relu: db = [[-0.20837892]]
\end{verbatim}

    \#\#\# 6.3 - L-Model Backward

Now you will implement the backward function for the whole network!

Recall that when you implemented the \texttt{L\_model\_forward}
function, at each iteration, you stored a cache which contains (X,W,b,
and z). In the back propagation module, you'll use those variables to
compute the gradients. Therefore, in the \texttt{L\_model\_backward}
function, you'll iterate through all the hidden layers backward,
starting from layer \(L\). On each step, you will use the cached values
for layer \(l\) to backpropagate through layer \(l\). Figure 5 below
shows the backward pass.

Figure 5: Backward pass

\textbf{Initializing backpropagation}:

To backpropagate through this network, you know that the output is:
\(A^{[L]} = \sigma(Z^{[L]})\). Your code thus needs to compute
\texttt{dAL} \(= \frac{\partial \mathcal{L}}{\partial A^{[L]}}\). To do
so, use this formula (derived using calculus which, again, you don't
need in-depth knowledge of!):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dAL }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{ (np.divide(Y, AL) }\OperatorTok{{-}}\NormalTok{ np.divide(}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ Y, }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ AL)) }\CommentTok{\# derivative of cost with respect to AL}
\end{Highlighting}
\end{Shaded}

You can then use this post-activation gradient \texttt{dAL} to keep
going backward. As seen in Figure 5, you can now feed in \texttt{dAL}
into the LINEAR-\textgreater SIGMOID backward function you implemented
(which will use the cached values stored by the L\_model\_forward
function).

After that, you will have to use a \texttt{for} loop to iterate through
all the other layers using the LINEAR-\textgreater RELU backward
function. You should store each dA, dW, and db in the grads dictionary.
To do so, use this formula :

\[grads["dW" + str(l)] = dW^{[l]}\tag{15} \]

For example, for \(l=3\) this would store \(dW^{[l]}\) in
\texttt{grads{[}"dW3"{]}}.

\#\#\# Exercise 9 - L\_model\_backward

Implement backpropagation for the \emph{{[}LINEAR-\textgreater RELU{]}
\(\times\) (L-1) -\textgreater{} LINEAR -\textgreater{} SIGMOID} model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: L\PYZus{}model\PYZus{}backward}

\PY{k}{def} \PY{n+nf}{L\PYZus{}model\PYZus{}backward}\PY{p}{(}\PY{n}{AL}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{caches}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement the backward propagation for the [LINEAR\PYZhy{}\PYZgt{}RELU] * (L\PYZhy{}1) \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} SIGMOID group}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    AL \PYZhy{}\PYZhy{} probability vector, output of the forward propagation (L\PYZus{}model\PYZus{}forward())}
\PY{l+s+sd}{    Y \PYZhy{}\PYZhy{} true \PYZdq{}label\PYZdq{} vector (containing 0 if non\PYZhy{}cat, 1 if cat)}
\PY{l+s+sd}{    caches \PYZhy{}\PYZhy{} list of caches containing:}
\PY{l+s+sd}{                every cache of linear\PYZus{}activation\PYZus{}forward() with \PYZdq{}relu\PYZdq{} (it\PYZsq{}s caches[l], for l in range(L\PYZhy{}1) i.e l = 0...L\PYZhy{}2)}
\PY{l+s+sd}{                the cache of linear\PYZus{}activation\PYZus{}forward() with \PYZdq{}sigmoid\PYZdq{} (it\PYZsq{}s caches[L\PYZhy{}1])}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    grads \PYZhy{}\PYZhy{} A dictionary with the gradients}
\PY{l+s+sd}{             grads[\PYZdq{}dA\PYZdq{} + str(l)] = ... }
\PY{l+s+sd}{             grads[\PYZdq{}dW\PYZdq{} + str(l)] = ...}
\PY{l+s+sd}{             grads[\PYZdq{}db\PYZdq{} + str(l)] = ... }
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{grads} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
    \PY{n}{L} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{caches}\PY{p}{)} \PY{c+c1}{\PYZsh{} the number of layers}
    \PY{n}{m} \PY{o}{=} \PY{n}{AL}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{Y} \PY{o}{=} \PY{n}{Y}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{AL}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{c+c1}{\PYZsh{} after this line, Y is the same shape as AL}
    
    \PY{c+c1}{\PYZsh{} Initializing the backpropagation}
    \PY{c+c1}{\PYZsh{}(1 line of code)}
    \PY{c+c1}{\PYZsh{} dAL = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{dAL} \PY{o}{=} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{divide}\PY{p}{(}\PY{n}{Y}\PY{p}{,} \PY{n}{AL}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{divide}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{Y}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{AL}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{current\PYZus{}cache} \PY{o}{=} \PY{n}{caches}\PY{p}{[}\PY{n}{L}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{;}
    \PY{n}{dA\PYZus{}prev\PYZus{}temp}\PY{p}{,} \PY{n}{dW\PYZus{}temp}\PY{p}{,} \PY{n}{db\PYZus{}temp} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}backward}\PY{p}{(}\PY{n}{dAL}\PY{p}{,} \PY{n}{current\PYZus{}cache}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{L}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{dA\PYZus{}prev\PYZus{}temp}
    \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{L}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{dW\PYZus{}temp}
    \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{L}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{db\PYZus{}temp}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{c+c1}{\PYZsh{} Loop from l=L\PYZhy{}2 to l=0}
    \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n+nb}{reversed}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{L}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{current\PYZus{}cache} \PY{o}{=} \PY{n}{caches}\PY{p}{[}\PY{n}{l}\PY{p}{]}\PY{p}{;}
        \PY{n}{dA\PYZus{}prev\PYZus{}temp}\PY{p}{,} \PY{n}{dW\PYZus{}temp}\PY{p}{,} \PY{n}{db\PYZus{}temp} \PY{o}{=} \PY{n}{linear\PYZus{}activation\PYZus{}backward}\PY{p}{(}\PY{n}{dA\PYZus{}prev\PYZus{}temp}\PY{p}{,} \PY{n}{current\PYZus{}cache}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{dA\PYZus{}prev\PYZus{}temp}
        \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{dW\PYZus{}temp}
        \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{db\PYZus{}temp}
        
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}

    \PY{k}{return} \PY{n}{grads}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}AL}\PY{p}{,} \PY{n}{t\PYZus{}Y\PYZus{}assess}\PY{p}{,} \PY{n}{t\PYZus{}caches} \PY{o}{=} \PY{n}{L\PYZus{}model\PYZus{}backward\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}
\PY{n}{grads} \PY{o}{=} \PY{n}{L\PYZus{}model\PYZus{}backward}\PY{p}{(}\PY{n}{t\PYZus{}AL}\PY{p}{,} \PY{n}{t\PYZus{}Y\PYZus{}assess}\PY{p}{,} \PY{n}{t\PYZus{}caches}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA0 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dA0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA1 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dA1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW1 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dW1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW2 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dW2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db1 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{db1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db2 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{db2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{L\PYZus{}model\PYZus{}backward\PYZus{}test}\PY{p}{(}\PY{n}{L\PYZus{}model\PYZus{}backward}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dA0 = [[ 0.          0.52257901]
 [ 0.         -0.3269206 ]
 [ 0.         -0.32070404]
 [ 0.         -0.74079187]]
dA1 = [[ 0.12913162 -0.44014127]
 [-0.14175655  0.48317296]
 [ 0.01663708 -0.05670698]]
dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]
 [0.         0.         0.         0.        ]
 [0.05283652 0.01005865 0.01777766 0.0135308 ]]
dW2 = [[-0.39202432 -0.13325855 -0.04601089]]
db1 = [[-0.22007063]
 [ 0.        ]
 [-0.02835349]]
db2 = [[0.15187861]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{Expected output:}

\begin{verbatim}
dA0 = [[ 0.          0.52257901]
 [ 0.         -0.3269206 ]
 [ 0.         -0.32070404]
 [ 0.         -0.74079187]]
dA1 = [[ 0.12913162 -0.44014127]
 [-0.14175655  0.48317296]
 [ 0.01663708 -0.05670698]]
dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]
 [0.         0.         0.         0.        ]
 [0.05283652 0.01005865 0.01777766 0.0135308 ]]
dW2 = [[-0.39202432 -0.13325855 -0.04601089]]
db1 = [[-0.22007063]
 [ 0.        ]
 [-0.02835349]]
db2 = [[0.15187861]]
\end{verbatim}

    \#\#\# 6.4 - Update Parameters

In this section, you'll update the parameters of the model, using
gradient descent:

\[ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{16}\]
\[ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{17}\]

where \(\alpha\) is the learning rate.

After computing the updated parameters, store them in the parameters
dictionary.

    \#\#\# Exercise 10 - update\_parameters

Implement \texttt{update\_parameters()} to update your parameters using
gradient descent.

\textbf{Instructions}: Update parameters using gradient descent on every
\(W^{[l]}\) and \(b^{[l]}\) for \(l = 1, 2, ..., L\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: update\PYZus{}parameters}

\PY{k}{def} \PY{n+nf}{update\PYZus{}parameters}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{grads}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Update parameters using gradient descent}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    params \PYZhy{}\PYZhy{} python dictionary containing your parameters }
\PY{l+s+sd}{    grads \PYZhy{}\PYZhy{} python dictionary containing your gradients, output of L\PYZus{}model\PYZus{}backward}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} python dictionary containing your updated parameters }
\PY{l+s+sd}{                  parameters[\PYZdq{}W\PYZdq{} + str(l)] = ... }
\PY{l+s+sd}{                  parameters[\PYZdq{}b\PYZdq{} + str(l)] = ...}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{parameters} \PY{o}{=} \PY{n}{params}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{L} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{parameters}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2} \PY{c+c1}{\PYZsh{} number of layers in the neural network}

    \PY{c+c1}{\PYZsh{} Update rule for each parameter. Use a for loop.}
    \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
    \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{L}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} parameters[\PYZdq{}W\PYZdq{} + str(l+1)] = ...}
        \PY{c+c1}{\PYZsh{} parameters[\PYZdq{}b\PYZdq{} + str(l+1)] = ...}
        \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
        \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
        \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{l}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{k}{return} \PY{n}{parameters}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}parameters}\PY{p}{,} \PY{n}{grads} \PY{o}{=} \PY{n}{update\PYZus{}parameters\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}
\PY{n}{t\PYZus{}parameters} \PY{o}{=} \PY{n}{update\PYZus{}parameters}\PY{p}{(}\PY{n}{t\PYZus{}parameters}\PY{p}{,} \PY{n}{grads}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}

\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1 = }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1 = }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2 = }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2 = }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{t\PYZus{}parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{update\PYZus{}parameters\PYZus{}test}\PY{p}{(}\PY{n}{update\PYZus{}parameters}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]]
b1 = [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]]
W2 = [[-0.55569196  0.0354055   1.32964895]]
b2 = [[-0.84610769]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \textbf{Expected output:}

\begin{verbatim}
W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]]
b1 = [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]]
W2 = [[-0.55569196  0.0354055   1.32964895]]
b2 = [[-0.84610769]]
\end{verbatim}

    \hypertarget{congratulations}{%
\subsubsection{Congratulations!}\label{congratulations}}

You've just implemented all the functions required for building a deep
neural network, including:

\begin{itemize}
\tightlist
\item
  Using non-linear units improve your model
\item
  Building a deeper neural network (with more than 1 hidden layer)
\item
  Implementing an easy-to-use neural network class
\end{itemize}

This was indeed a long assignment, but the next part of the assignment
is easier. ;)

In the next assignment, you'll be putting all these together to build
two models:

\begin{itemize}
\tightlist
\item
  A two-layer neural network
\item
  An L-layer neural network
\end{itemize}

You will in fact use these models to classify cat vs non-cat images!
(Meow!) Great work and see you next time.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
